{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Continuous Bag of Words (CBOW) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/kienmn/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/kienmn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/kienmn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_document(doc):\n",
    "    # Lowercase and remove special characters/whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # Tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # Filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Recreate document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible = gutenberg.sents('bible-kjv.txt')\n",
    "remove_term = punctuation + '0123456789'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[', 'The', 'King', 'James', 'Bible', ']'], ['The', 'Old', 'Testament', 'of', 'the', 'King', 'James', 'Bible'], ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~0123456789'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_bible = [[word.lower() for word in sent if word not in remove_term] for sent in bible]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'king', 'james', 'bible'],\n",
       " ['the', 'old', 'testament', 'of', 'the', 'king', 'james', 'bible'],\n",
       " ['the', 'first', 'book', 'of', 'moses', 'called', 'genesis']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_bible[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_bible = [' '.join(tok_sent) for tok_sent in norm_bible]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the king james bible',\n",
       " 'the old testament of the king james bible',\n",
       " 'the first book of moses called genesis']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_bible[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_bible = filter(None, normalize_corpus(norm_bible))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_bible = [tok_sent for tok_sent in norm_bible if len(tok_sent.split()) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['king james bible',\n",
       " 'old testament king james bible',\n",
       " 'first book moses called genesis',\n",
       " 'beginning god created heaven earth',\n",
       " 'earth without form void darkness upon face deep',\n",
       " 'spirit god moved upon face waters',\n",
       " 'god said let light light',\n",
       " 'god saw light good god divided light darkness',\n",
       " 'god called light day darkness called night',\n",
       " 'evening morning first day']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_bible[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 30103\n",
      "Sample line: ['1', ':', '6', 'And', 'God', 'said', ',', 'Let', 'there', 'be', 'a', 'firmament', 'in', 'the', 'midst', 'of', 'the', 'waters', ',', 'and', 'let', 'it', 'divide', 'the', 'waters', 'from', 'the', 'waters', '.']\n",
      "Processed line: god said let firmament midst waters let divide waters waters\n"
     ]
    }
   ],
   "source": [
    "print('Total lines:', len(bible))\n",
    "print('Sample line:', bible[10])\n",
    "print('Processed line:', norm_bible[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Continuous Bag of Words (CBOW) Model\n",
    "### Building the corpus vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text, sequence\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(norm_bible)\n",
    "word2id = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary of unique words\n",
    "word2id['PAD'] = 0\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[13, 1154, 5766], [154, 2450, 13, 1154, 5766], [132, 310, 63, 86, 8480]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wids[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2id)\n",
    "embed_size = 100\n",
    "window_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 12425\n",
      "Vocabulary Sample: [('shall', 1), ('unto', 2), ('lord', 3), ('thou', 4), ('thy', 5), ('god', 6), ('ye', 7), ('said', 8), ('thee', 9), ('upon', 10)]\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a CBOW (context, target) generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context (X): ['old', 'testament', 'james', 'bible'] -> Target (Y): king\n",
      "Context (X): ['first', 'book', 'called', 'genesis'] -> Target (Y): moses\n",
      "Context (X): ['beginning', 'god', 'heaven', 'earth'] -> Target (Y): created\n",
      "Context (X): ['earth', 'without', 'void', 'darkness'] -> Target (Y): form\n",
      "Context (X): ['without', 'form', 'darkness', 'upon'] -> Target (Y): void\n",
      "Context (X): ['form', 'void', 'upon', 'face'] -> Target (Y): darkness\n",
      "Context (X): ['void', 'darkness', 'face', 'deep'] -> Target (Y): upon\n",
      "Context (X): ['spirit', 'god', 'upon', 'face'] -> Target (Y): moved\n",
      "Context (X): ['god', 'moved', 'face', 'waters'] -> Target (Y): upon\n",
      "Context (X): ['god', 'said', 'light', 'light'] -> Target (Y): let\n",
      "Context (X): ['god', 'saw', 'good', 'god'] -> Target (Y): light\n"
     ]
    }
   ],
   "source": [
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    context_length = window_size * 2\n",
    "    for words in corpus:\n",
    "        sentence_length = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            context_words = []\n",
    "            label_word = []\n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            \n",
    "            context_words.append([words[i]\n",
    "                                  for i in range(start, end)\n",
    "                                  if 0 <= i < sentence_length\n",
    "                                  and i != index])\n",
    "            label_word.append(word)\n",
    "            x = sequence.pad_sequences(context_words, maxlen=context_length)\n",
    "            y = np_utils.to_categorical(label_word, vocab_size)\n",
    "            yield(x, y)\n",
    "            \n",
    "# Test this out for some samples\n",
    "i = 0\n",
    "for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "    if 0 not in x[0]:\n",
    "        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
    "\n",
    "        if i == 10:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the CBOW model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building CBOW architecture\n",
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "cbow.add(Dense(vocab_size, activation='softmax'))\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4, 100)            1242500   \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12425)             1254925   \n",
      "=================================================================\n",
      "Total params: 2,497,425\n",
      "Trainable params: 2,497,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cbow.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"274pt\" viewBox=\"0.00 0.00 240.00 304.00\" width=\"217pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(.9028 .9028) rotate(0) translate(4 300)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-300 236,-300 236,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140137233947560 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140137233947560</title>\n",
       "<polygon fill=\"none\" points=\"15.5,-249.5 15.5,-295.5 216.5,-295.5 216.5,-249.5 15.5,-249.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"54\" y=\"-268.8\">InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"92.5,-249.5 92.5,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"120\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"92.5,-272.5 147.5,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"120\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"147.5,-249.5 147.5,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"182\" y=\"-280.3\">(None, 4)</text>\n",
       "<polyline fill=\"none\" points=\"147.5,-272.5 216.5,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"182\" y=\"-257.3\">(None, 4)</text>\n",
       "</g>\n",
       "<!-- 140137233948624 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140137233948624</title>\n",
       "<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 232,-212.5 232,-166.5 0,-166.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"40\" y=\"-185.8\">Embedding</text>\n",
       "<polyline fill=\"none\" points=\"80,-166.5 80,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"80,-189.5 135,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"107.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"135,-166.5 135,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183.5\" y=\"-197.3\">(None, 4)</text>\n",
       "<polyline fill=\"none\" points=\"135,-189.5 232,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183.5\" y=\"-174.3\">(None, 4, 100)</text>\n",
       "</g>\n",
       "<!-- 140137233947560&#45;&gt;140137233948624 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140137233947560-&gt;140137233948624</title>\n",
       "<path d=\"M116,-249.3799C116,-241.1745 116,-231.7679 116,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"119.5001,-222.784 116,-212.784 112.5001,-222.784 119.5001,-222.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140137233944816 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140137233944816</title>\n",
       "<polygon fill=\"none\" points=\"9,-83.5 9,-129.5 223,-129.5 223,-83.5 9,-83.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"40\" y=\"-102.8\">Lambda</text>\n",
       "<polyline fill=\"none\" points=\"71,-83.5 71,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"98.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"71,-106.5 126,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"98.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"126,-83.5 126,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"174.5\" y=\"-114.3\">(None, 4, 100)</text>\n",
       "<polyline fill=\"none\" points=\"126,-106.5 223,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"174.5\" y=\"-91.3\">(None, 100)</text>\n",
       "</g>\n",
       "<!-- 140137233948624&#45;&gt;140137233944816 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140137233948624-&gt;140137233944816</title>\n",
       "<path d=\"M116,-166.3799C116,-158.1745 116,-148.7679 116,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"119.5001,-139.784 116,-129.784 112.5001,-139.784 119.5001,-139.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140137233945880 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140137233945880</title>\n",
       "<polygon fill=\"none\" points=\"15.5,-.5 15.5,-46.5 216.5,-46.5 216.5,-.5 15.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"40.5\" y=\"-19.8\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"65.5,-.5 65.5,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"93\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"65.5,-23.5 120.5,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"93\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"120.5,-.5 120.5,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"168.5\" y=\"-31.3\">(None, 100)</text>\n",
       "<polyline fill=\"none\" points=\"120.5,-23.5 216.5,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"168.5\" y=\"-8.3\">(None, 12425)</text>\n",
       "</g>\n",
       "<!-- 140137233944816&#45;&gt;140137233945880 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140137233944816-&gt;140137233945880</title>\n",
       "<path d=\"M116,-83.3799C116,-75.1745 116,-65.7679 116,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"119.5001,-56.784 116,-46.784 112.5001,-56.784 119.5001,-56.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize model structure\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(cbow, show_shapes=True, show_layer_names=False, rankdir='TB', dpi=65).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Take quite long time to train\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    loss = 0.\n",
    "    i = 0\n",
    "    for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "        i += 1\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "        if i % 100000 == 0:\n",
    "            print('Processed {} (context, word) pairs'.format(i))\n",
    "\n",
    "    print('Epoch:', epoch, '\\tLoss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12424, 100)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = cbow.get_weights()[0]\n",
    "weights = weights[1:]   # Note: weights[0] is for PAD.\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>shall</td>\n",
       "      <td>0.032743</td>\n",
       "      <td>0.004620</td>\n",
       "      <td>0.033932</td>\n",
       "      <td>-0.049682</td>\n",
       "      <td>-0.020793</td>\n",
       "      <td>0.029174</td>\n",
       "      <td>-0.040691</td>\n",
       "      <td>-0.036483</td>\n",
       "      <td>-0.027343</td>\n",
       "      <td>0.025122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039270</td>\n",
       "      <td>0.013598</td>\n",
       "      <td>0.038559</td>\n",
       "      <td>-0.006084</td>\n",
       "      <td>-0.040633</td>\n",
       "      <td>-0.040619</td>\n",
       "      <td>-0.011261</td>\n",
       "      <td>-0.005539</td>\n",
       "      <td>0.040170</td>\n",
       "      <td>-0.040601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>unto</td>\n",
       "      <td>-0.003183</td>\n",
       "      <td>-0.030217</td>\n",
       "      <td>0.024062</td>\n",
       "      <td>-0.033074</td>\n",
       "      <td>0.038236</td>\n",
       "      <td>0.017084</td>\n",
       "      <td>0.022587</td>\n",
       "      <td>-0.032710</td>\n",
       "      <td>-0.020847</td>\n",
       "      <td>-0.012313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012389</td>\n",
       "      <td>-0.019922</td>\n",
       "      <td>-0.018460</td>\n",
       "      <td>-0.036715</td>\n",
       "      <td>-0.009048</td>\n",
       "      <td>-0.034876</td>\n",
       "      <td>-0.003414</td>\n",
       "      <td>-0.022669</td>\n",
       "      <td>-0.009553</td>\n",
       "      <td>-0.037421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>lord</td>\n",
       "      <td>0.046552</td>\n",
       "      <td>0.009463</td>\n",
       "      <td>-0.033299</td>\n",
       "      <td>-0.016541</td>\n",
       "      <td>-0.017908</td>\n",
       "      <td>-0.019655</td>\n",
       "      <td>-0.025923</td>\n",
       "      <td>-0.018011</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>-0.034221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>-0.038869</td>\n",
       "      <td>-0.044725</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>-0.042801</td>\n",
       "      <td>0.020157</td>\n",
       "      <td>-0.034456</td>\n",
       "      <td>-0.011043</td>\n",
       "      <td>0.037131</td>\n",
       "      <td>0.040633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>thou</td>\n",
       "      <td>0.025958</td>\n",
       "      <td>0.019875</td>\n",
       "      <td>0.006499</td>\n",
       "      <td>-0.006116</td>\n",
       "      <td>-0.043054</td>\n",
       "      <td>0.027911</td>\n",
       "      <td>0.034857</td>\n",
       "      <td>-0.033050</td>\n",
       "      <td>-0.002873</td>\n",
       "      <td>0.030157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021045</td>\n",
       "      <td>0.029871</td>\n",
       "      <td>0.024505</td>\n",
       "      <td>0.009087</td>\n",
       "      <td>0.044203</td>\n",
       "      <td>-0.040807</td>\n",
       "      <td>0.001219</td>\n",
       "      <td>0.003751</td>\n",
       "      <td>-0.044954</td>\n",
       "      <td>0.046748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>thy</td>\n",
       "      <td>0.017528</td>\n",
       "      <td>0.005626</td>\n",
       "      <td>-0.026871</td>\n",
       "      <td>0.039219</td>\n",
       "      <td>0.036824</td>\n",
       "      <td>0.022392</td>\n",
       "      <td>0.005548</td>\n",
       "      <td>0.044245</td>\n",
       "      <td>-0.013060</td>\n",
       "      <td>0.049354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003166</td>\n",
       "      <td>-0.021126</td>\n",
       "      <td>0.047324</td>\n",
       "      <td>0.028593</td>\n",
       "      <td>0.008060</td>\n",
       "      <td>0.018543</td>\n",
       "      <td>-0.040648</td>\n",
       "      <td>0.013222</td>\n",
       "      <td>-0.018054</td>\n",
       "      <td>-0.012824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "shall  0.032743  0.004620  0.033932 -0.049682 -0.020793  0.029174 -0.040691   \n",
       "unto  -0.003183 -0.030217  0.024062 -0.033074  0.038236  0.017084  0.022587   \n",
       "lord   0.046552  0.009463 -0.033299 -0.016541 -0.017908 -0.019655 -0.025923   \n",
       "thou   0.025958  0.019875  0.006499 -0.006116 -0.043054  0.027911  0.034857   \n",
       "thy    0.017528  0.005626 -0.026871  0.039219  0.036824  0.022392  0.005548   \n",
       "\n",
       "             7         8         9   ...        90        91        92  \\\n",
       "shall -0.036483 -0.027343  0.025122  ...  0.039270  0.013598  0.038559   \n",
       "unto  -0.032710 -0.020847 -0.012313  ...  0.012389 -0.019922 -0.018460   \n",
       "lord  -0.018011  0.000756 -0.034221  ...  0.001636 -0.038869 -0.044725   \n",
       "thou  -0.033050 -0.002873  0.030157  ...  0.021045  0.029871  0.024505   \n",
       "thy    0.044245 -0.013060  0.049354  ...  0.003166 -0.021126  0.047324   \n",
       "\n",
       "             93        94        95        96        97        98        99  \n",
       "shall -0.006084 -0.040633 -0.040619 -0.011261 -0.005539  0.040170 -0.040601  \n",
       "unto  -0.036715 -0.009048 -0.034876 -0.003414 -0.022669 -0.009553 -0.037421  \n",
       "lord   0.004977 -0.042801  0.020157 -0.034456 -0.011043  0.037131  0.040633  \n",
       "thou   0.009087  0.044203 -0.040807  0.001219  0.003751 -0.044954  0.046748  \n",
       "thy    0.028593  0.008060  0.018543 -0.040648  0.013222 -0.018054 -0.012824  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(weights, index=list(id2word.values())[:-1]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12424, 12424)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# Compute pairwise distance matrix\n",
    "distance_matrix = euclidean_distances(weights)\n",
    "print(distance_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term] - 1].argsort()[1: 6] + 1]\n",
    "                for search_term in ['god', 'jesus', 'noah', 'egypt', 'john', 'gospel', 'moses', 'famine']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'god': ['malchielites', 'finished', 'highest', 'treasury', 'spiritually'],\n",
       " 'jesus': ['meah', 'rolling', 'beareth', 'invited', 'roasted'],\n",
       " 'noah': ['evi', 'stephen', 'withholdeth', 'jaalah', 'unwittingly'],\n",
       " 'egypt': ['spin', 'falling', 'uprightness', 'bridle', 'question'],\n",
       " 'john': ['scythian', 'shetharboznai', 'ripe', 'wonderful', 'marketplace'],\n",
       " 'gospel': ['emmanuel', 'ought', 'jerah', 'patrimony', 'angels'],\n",
       " 'moses': ['rebuketh', 'bodies', 'frame', 'zephaniah', 'lock'],\n",
       " 'famine': ['pointed', 'doorkeeper', 'bethany', 'bushy', 'shunem']}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
